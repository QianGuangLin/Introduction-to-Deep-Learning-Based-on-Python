{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详情请查阅《[图灵程序设计丛书].深度学习入门：基于Python的理论与实现》\n",
    "\n",
    "Chapter 07 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何计算输出大小。\n",
    "这里，假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过式(7.1)进行计算。\n",
    "\n",
    "$ \\quad \\quad$ $ OH = \\frac{H+2P-FH}{S} + 1 $\n",
    "\n",
    "$ \\quad \\quad$ $ OW = \\frac{H+2P-FW}{S} + 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4  卷积层和池化层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.4.1  4维数组\n",
    "\n",
    "CNN中各层间传递的数据是4维数据。所谓4维数据，比如数据的形状是(10, 1, 28, 28)，则它对应10个高为28、长为28、通道为1的数据。\n",
    "\n",
    "用Python来实现的话，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.00228116, 0.17797381, 0.44786079, ..., 0.43332458,\n",
       "          0.70711057, 0.36662378],\n",
       "         [0.20432805, 0.89467903, 0.77637355, ..., 0.99589978,\n",
       "          0.98098646, 0.10105912],\n",
       "         [0.42590004, 0.65507664, 0.64996883, ..., 0.61184032,\n",
       "          0.61837844, 0.96498999],\n",
       "         ...,\n",
       "         [0.60643704, 0.47289697, 0.59351538, ..., 0.5614185 ,\n",
       "          0.48181903, 0.98278272],\n",
       "         [0.37742237, 0.35465636, 0.9968071 , ..., 0.36087676,\n",
       "          0.68203441, 0.59554306],\n",
       "         [0.95558291, 0.44657116, 0.60054205, ..., 0.79611716,\n",
       "          0.73069198, 0.15868928]]],\n",
       "\n",
       "\n",
       "       [[[0.00313961, 0.24555281, 0.14975537, ..., 0.86851542,\n",
       "          0.36827554, 0.81655498],\n",
       "         [0.32795704, 0.04583974, 0.84562266, ..., 0.34225748,\n",
       "          0.73880846, 0.0827138 ],\n",
       "         [0.50915461, 0.55810557, 0.80517262, ..., 0.61910997,\n",
       "          0.67105615, 0.2702453 ],\n",
       "         ...,\n",
       "         [0.5908565 , 0.41690774, 0.34028612, ..., 0.38904613,\n",
       "          0.4852104 , 0.06356429],\n",
       "         [0.62143588, 0.11295417, 0.88941932, ..., 0.9848436 ,\n",
       "          0.21981433, 0.66862593],\n",
       "         [0.48824497, 0.15250813, 0.70830934, ..., 0.63296268,\n",
       "          0.8270572 , 0.51831512]]],\n",
       "\n",
       "\n",
       "       [[[0.34025869, 0.17850123, 0.91719366, ..., 0.99519785,\n",
       "          0.77562974, 0.44055403],\n",
       "         [0.86378038, 0.37852961, 0.46211993, ..., 0.56391757,\n",
       "          0.40671025, 0.29493985],\n",
       "         [0.84876216, 0.02058609, 0.24065721, ..., 0.01788067,\n",
       "          0.09432056, 0.13299012],\n",
       "         ...,\n",
       "         [0.24149507, 0.07702361, 0.60053521, ..., 0.7287012 ,\n",
       "          0.42648101, 0.78297064],\n",
       "         [0.76466666, 0.00822748, 0.84677937, ..., 0.59388086,\n",
       "          0.98046982, 0.48680628],\n",
       "         [0.34447164, 0.01710189, 0.12940168, ..., 0.53133441,\n",
       "          0.89114994, 0.44398464]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.03098419, 0.61728008, 0.89685011, ..., 0.83691904,\n",
       "          0.37657126, 0.3789467 ],\n",
       "         [0.65334646, 0.45935989, 0.24729266, ..., 0.56570423,\n",
       "          0.0033381 , 0.24569496],\n",
       "         [0.66284126, 0.6731739 , 0.48319918, ..., 0.49015963,\n",
       "          0.46719827, 0.46818224],\n",
       "         ...,\n",
       "         [0.66013952, 0.67518179, 0.28626271, ..., 0.41381504,\n",
       "          0.36513886, 0.20553861],\n",
       "         [0.35089826, 0.23975807, 0.99482127, ..., 0.48579511,\n",
       "          0.1438449 , 0.39811331],\n",
       "         [0.30919412, 0.15702278, 0.9813666 , ..., 0.51908732,\n",
       "          0.57933078, 0.2373796 ]]],\n",
       "\n",
       "\n",
       "       [[[0.74021156, 0.0246621 , 0.79873168, ..., 0.75932496,\n",
       "          0.31715482, 0.40265103],\n",
       "         [0.16312721, 0.2187897 , 0.07680952, ..., 0.79284528,\n",
       "          0.37419766, 0.95027637],\n",
       "         [0.13192806, 0.72429948, 0.64778189, ..., 0.34720563,\n",
       "          0.94845385, 0.39581917],\n",
       "         ...,\n",
       "         [0.5222681 , 0.85948677, 0.97783566, ..., 0.86289059,\n",
       "          0.22569672, 0.40632462],\n",
       "         [0.07263566, 0.34022879, 0.4662413 , ..., 0.22237567,\n",
       "          0.98335473, 0.61504604],\n",
       "         [0.49744385, 0.85029578, 0.06702835, ..., 0.14456269,\n",
       "          0.20201648, 0.30020039]]],\n",
       "\n",
       "\n",
       "       [[[0.4483045 , 0.43147112, 0.72202008, ..., 0.88626532,\n",
       "          0.80479313, 0.06121998],\n",
       "         [0.96765145, 0.53233714, 0.01940551, ..., 0.10033978,\n",
       "          0.35304304, 0.57169108],\n",
       "         [0.05156428, 0.74584885, 0.16013885, ..., 0.54593751,\n",
       "          0.91732461, 0.90299803],\n",
       "         ...,\n",
       "         [0.75596686, 0.63745379, 0.18704944, ..., 0.27310362,\n",
       "          0.9755413 , 0.87300683],\n",
       "         [0.09409937, 0.60232136, 0.72148256, ..., 0.37272195,\n",
       "          0.23165347, 0.56879999],\n",
       "         [0.86663943, 0.3905001 , 0.95935148, ..., 0.36508284,\n",
       "          0.2765494 , 0.21239614]]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28) # 随机生成数据\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n",
      "(1, 28, 28)\n",
      "(1, 28, 28)\n",
      "[[2.28115632e-03 1.77973814e-01 4.47860792e-01 2.45799501e-02\n",
      "  5.00818223e-02 9.46299830e-01 8.23998383e-01 6.85432259e-01\n",
      "  7.20961508e-01 1.09540463e-01 9.42246264e-01 6.39173271e-01\n",
      "  1.00530956e-01 3.50320417e-01 6.29666193e-01 1.24852068e-01\n",
      "  4.48512457e-01 3.23888900e-01 8.15368574e-01 8.60060962e-01\n",
      "  2.92915815e-01 7.13657251e-01 6.51453630e-01 6.91454157e-01\n",
      "  7.82541727e-01 4.33324585e-01 7.07110574e-01 3.66623775e-01]\n",
      " [2.04328052e-01 8.94679032e-01 7.76373552e-01 6.30610631e-01\n",
      "  3.58164262e-01 7.20659470e-01 9.47383078e-01 3.54165486e-01\n",
      "  8.35061925e-01 2.40385130e-01 6.49023215e-02 7.81415535e-01\n",
      "  7.55200234e-01 6.69937168e-01 2.55012284e-01 9.13123803e-01\n",
      "  5.25382653e-01 1.92088311e-01 4.63251705e-01 9.23490391e-01\n",
      "  4.28000384e-01 2.83647166e-01 9.46561896e-01 7.91150571e-01\n",
      "  4.56058323e-01 9.95899782e-01 9.80986461e-01 1.01059117e-01]\n",
      " [4.25900044e-01 6.55076640e-01 6.49968826e-01 8.01480894e-01\n",
      "  1.26720152e-01 6.50466208e-02 2.26560822e-01 8.55120352e-01\n",
      "  1.88309178e-01 2.29270386e-01 3.68751149e-01 7.43407005e-01\n",
      "  5.17650705e-01 3.51946211e-01 1.92570919e-01 4.56507414e-01\n",
      "  4.94644749e-01 8.69769788e-01 3.18779608e-01 8.36681577e-01\n",
      "  5.41458182e-01 2.47103327e-01 6.93492595e-01 4.85912172e-02\n",
      "  9.62149045e-01 6.11840324e-01 6.18378439e-01 9.64989989e-01]\n",
      " [8.54879532e-01 1.00312875e-01 6.38990862e-01 7.23645247e-01\n",
      "  7.06661322e-01 4.36587275e-01 8.53911445e-01 3.74054639e-01\n",
      "  5.12281028e-01 4.42931063e-01 5.88736408e-01 3.37894563e-01\n",
      "  2.47478696e-01 1.40714918e-01 1.06848749e-01 7.53067380e-01\n",
      "  3.67822629e-01 7.26029441e-02 7.62201812e-01 7.15476869e-01\n",
      "  2.82743748e-01 3.19422548e-01 6.72165095e-02 6.69885967e-01\n",
      "  5.03812346e-01 3.10669425e-01 1.11722718e-01 6.90170309e-01]\n",
      " [2.86346906e-01 7.61772722e-01 3.68336320e-01 7.32450739e-01\n",
      "  3.20143317e-01 8.21249757e-01 4.64934421e-01 7.77894146e-01\n",
      "  7.77468496e-01 7.56228664e-01 6.76000681e-01 1.50015303e-01\n",
      "  2.85232420e-01 4.20240542e-01 1.79330979e-01 2.84876217e-01\n",
      "  2.90835232e-01 4.39089140e-01 6.50448928e-01 2.14641921e-01\n",
      "  4.75667852e-01 1.65763908e-01 5.02588297e-01 4.06249317e-01\n",
      "  4.44706583e-01 3.92874225e-01 9.06074512e-01 4.64285733e-01]\n",
      " [1.20141642e-01 1.12786740e-01 2.30158947e-01 9.65541294e-01\n",
      "  2.22717304e-01 2.49868539e-01 5.92064589e-01 5.20508250e-02\n",
      "  3.85221106e-01 8.90361303e-02 2.49818930e-01 7.73986220e-01\n",
      "  4.99473945e-01 9.69446124e-01 2.57453072e-01 9.25345791e-01\n",
      "  4.20495159e-01 9.42206016e-01 1.96176739e-01 8.90522754e-01\n",
      "  5.72015174e-01 7.61440465e-01 3.97732841e-01 2.68343354e-01\n",
      "  2.95406736e-01 8.98779593e-01 6.58835044e-01 4.92387764e-01]\n",
      " [4.98948958e-01 9.42496284e-02 9.10604345e-01 4.43855296e-01\n",
      "  6.32296233e-01 8.73564809e-01 4.08585814e-01 6.95405689e-01\n",
      "  7.38688511e-01 1.50143594e-01 7.87873180e-01 9.78901108e-01\n",
      "  4.06028605e-01 6.57199952e-01 6.54671065e-01 6.10580500e-01\n",
      "  8.39100261e-01 4.53708785e-01 2.43632753e-01 6.97708709e-01\n",
      "  8.75959987e-01 7.08432292e-01 1.70268590e-02 8.84270614e-01\n",
      "  8.99308682e-01 8.13145355e-01 2.48083624e-01 1.54444808e-01]\n",
      " [8.18267054e-01 5.82380162e-01 4.95990017e-01 7.35293342e-01\n",
      "  8.44934777e-01 6.05997618e-01 3.66567545e-01 3.60215387e-01\n",
      "  9.57527652e-01 8.55832655e-01 3.21732327e-01 3.73094225e-01\n",
      "  3.67735065e-01 1.04995261e-01 2.99851090e-01 3.96705969e-01\n",
      "  5.91653352e-01 3.24883375e-01 8.09150927e-01 5.74214029e-01\n",
      "  8.57379338e-01 4.39167771e-01 6.31175032e-01 5.83463782e-01\n",
      "  8.52897102e-01 6.74231011e-01 8.97183227e-01 6.28924653e-01]\n",
      " [3.87820857e-01 5.73211552e-01 8.29898830e-01 9.16460830e-02\n",
      "  7.57506623e-01 7.88728432e-01 4.54336272e-01 4.11582284e-01\n",
      "  6.57462436e-01 2.61103048e-01 1.82696009e-01 3.72739225e-01\n",
      "  4.24822149e-01 2.35109067e-01 7.60482372e-02 3.29626632e-02\n",
      "  4.93548480e-01 9.44202527e-01 6.43680644e-01 5.49359993e-01\n",
      "  6.16608344e-01 4.35759645e-01 2.28426755e-01 5.71605594e-01\n",
      "  3.02667651e-01 9.70224349e-01 2.05686324e-01 4.18177917e-01]\n",
      " [7.08102872e-01 8.92659383e-01 9.23673710e-01 9.85712491e-01\n",
      "  5.07448009e-01 6.75847849e-01 5.26279695e-02 1.65043286e-01\n",
      "  1.74460319e-01 4.68037028e-01 1.14508673e-01 2.84873431e-01\n",
      "  7.87784379e-01 1.64089982e-01 5.82506192e-01 2.03713704e-01\n",
      "  9.24884741e-01 8.87328837e-01 6.62971902e-01 4.57182121e-01\n",
      "  5.81720201e-01 8.00396271e-01 1.59220347e-01 9.07457992e-01\n",
      "  2.78391159e-01 9.84529736e-01 7.42515349e-01 9.39271380e-01]\n",
      " [2.72970254e-01 2.78065522e-01 8.66037917e-01 5.08499831e-01\n",
      "  7.25813083e-01 1.80280869e-02 8.72517760e-01 6.68114759e-02\n",
      "  7.75814889e-01 6.86204459e-01 2.26842894e-02 3.93379437e-01\n",
      "  8.24818554e-01 9.75647408e-02 4.60977116e-01 5.76634125e-01\n",
      "  5.82729372e-01 2.34710423e-01 2.33065103e-01 2.31804188e-01\n",
      "  3.89473640e-01 9.64785081e-01 5.50973065e-01 7.02619352e-01\n",
      "  2.31935400e-01 3.11280754e-01 9.68747487e-01 6.93543211e-01]\n",
      " [8.50960588e-01 3.18331751e-01 8.15698214e-01 8.92775471e-01\n",
      "  9.91306815e-01 1.80471583e-01 7.56813623e-01 4.87259815e-01\n",
      "  3.56946948e-01 2.88197168e-01 4.31017040e-01 3.59978039e-01\n",
      "  7.24211269e-01 7.94627685e-01 4.65062364e-01 6.87802904e-01\n",
      "  2.27358989e-01 3.13525026e-01 6.89537255e-02 8.48019803e-01\n",
      "  8.91202183e-01 4.24086342e-02 2.18209551e-02 3.01503284e-01\n",
      "  8.26291919e-01 7.83573148e-01 1.75625043e-01 9.29876373e-01]\n",
      " [6.47780654e-01 4.99436371e-01 5.11133549e-01 9.68841540e-01\n",
      "  5.09334722e-01 4.94704397e-01 1.31314685e-01 3.88999040e-01\n",
      "  6.72035178e-01 1.37937906e-01 5.05678852e-01 3.33285673e-01\n",
      "  4.35653516e-01 8.09093781e-01 2.92529833e-01 7.26031543e-01\n",
      "  2.64893584e-03 9.86499150e-01 2.50901497e-01 6.47987711e-01\n",
      "  1.60510822e-02 1.37064532e-01 9.05107726e-01 8.32284094e-01\n",
      "  2.48063833e-01 6.86856044e-01 5.55302688e-01 7.13283778e-01]\n",
      " [9.30544834e-01 1.07179994e-01 9.34829705e-01 7.54887581e-01\n",
      "  5.11981031e-02 1.25465738e-01 1.46048412e-01 5.28095971e-01\n",
      "  1.08996031e-01 7.39391174e-01 7.01553082e-01 4.31384081e-01\n",
      "  7.10512716e-01 3.55114072e-01 3.92490277e-01 8.26378877e-01\n",
      "  6.07041329e-01 2.78337158e-01 1.65365716e-01 5.02127298e-01\n",
      "  4.16466686e-01 9.67860844e-01 7.52949042e-01 6.23588925e-01\n",
      "  5.69129480e-01 7.40143262e-01 1.14520063e-01 8.26122006e-01]\n",
      " [4.16376081e-01 7.60317542e-01 9.59481833e-01 3.09743355e-02\n",
      "  6.25993087e-01 6.33553475e-01 8.62766612e-02 6.44826911e-01\n",
      "  8.20657410e-01 7.61070538e-01 9.33183443e-01 7.24659582e-01\n",
      "  6.36734012e-01 7.79032588e-01 7.79594480e-01 5.06875112e-01\n",
      "  1.97003824e-01 4.39230374e-01 3.41545361e-02 4.61169812e-02\n",
      "  7.86877610e-01 2.39017511e-01 9.16759666e-01 1.99473146e-01\n",
      "  5.90621577e-01 5.12028759e-01 7.01667381e-01 4.78311758e-03]\n",
      " [1.30727073e-01 7.83630211e-02 4.81657302e-01 7.50055040e-01\n",
      "  1.43500587e-01 2.90693934e-01 6.72215656e-01 3.68915311e-01\n",
      "  6.10538814e-01 3.96944153e-01 3.95830342e-01 6.91525903e-01\n",
      "  8.25708879e-01 6.83646979e-01 9.02872302e-01 6.94410721e-01\n",
      "  1.17980376e-01 5.75955641e-01 8.72573551e-01 3.43270364e-01\n",
      "  7.68201147e-01 4.92564196e-04 8.64797040e-01 3.01787267e-01\n",
      "  8.49772831e-01 9.59935323e-01 3.27876908e-02 2.90675371e-01]\n",
      " [4.56555625e-01 7.86602401e-01 3.14675724e-01 3.24405479e-02\n",
      "  6.68247009e-01 1.97543460e-01 6.78086917e-01 3.40631215e-02\n",
      "  6.72127176e-01 7.11634133e-01 2.38174019e-01 6.16008502e-01\n",
      "  7.42307389e-01 3.62114921e-01 3.07065281e-03 8.92209584e-01\n",
      "  8.26888661e-01 1.00641460e-01 6.93910654e-01 1.90584277e-01\n",
      "  1.55316796e-01 6.15938983e-01 7.25402758e-01 1.48699104e-01\n",
      "  1.23715623e-03 7.81215542e-01 2.56411881e-01 8.70663823e-01]\n",
      " [3.57713230e-01 5.70973060e-01 2.28658042e-01 6.59095458e-01\n",
      "  5.96623344e-01 4.09775740e-01 7.07203543e-01 4.12865550e-01\n",
      "  1.40697579e-01 8.04749695e-01 8.31688907e-01 8.75951794e-01\n",
      "  7.04474116e-02 8.83826342e-01 9.24645443e-01 4.57669822e-01\n",
      "  4.90613830e-01 1.82964383e-01 4.83840770e-01 9.05442892e-01\n",
      "  2.55188107e-01 8.20312183e-01 2.67773179e-01 9.50646472e-01\n",
      "  2.73962232e-01 8.03287960e-01 2.66477245e-01 3.56400243e-01]\n",
      " [2.69001056e-01 4.32264015e-01 7.04549368e-01 2.85846586e-03\n",
      "  2.42445342e-01 5.97058853e-01 7.61009287e-01 7.47966757e-01\n",
      "  8.02817057e-01 1.62010297e-01 5.42246436e-01 2.72244912e-01\n",
      "  9.35787797e-01 4.20903738e-01 6.70850140e-01 8.75705373e-01\n",
      "  9.95214712e-01 7.24037574e-01 5.82474672e-01 9.59730619e-01\n",
      "  1.64451165e-01 5.78676091e-01 8.39937334e-01 8.40583240e-01\n",
      "  2.56216689e-01 9.00763744e-01 7.97523434e-01 8.91100869e-01]\n",
      " [7.63382258e-01 5.40813213e-01 2.13379518e-01 9.99725614e-01\n",
      "  4.23994271e-01 4.32653293e-01 9.84459549e-01 4.17165043e-01\n",
      "  7.81896400e-01 5.19401637e-01 1.10547682e-01 4.98162850e-01\n",
      "  4.64263729e-01 5.00318342e-01 9.42755271e-01 3.24985195e-02\n",
      "  6.14016930e-02 4.13150806e-01 4.84199939e-01 6.61020244e-01\n",
      "  7.72416513e-01 7.92606673e-01 3.71933879e-01 2.60804423e-01\n",
      "  3.45465471e-01 6.19110997e-01 3.54238994e-01 3.05856689e-01]\n",
      " [3.47583624e-01 1.07747321e-01 4.90876763e-01 4.12819254e-01\n",
      "  1.36629348e-01 8.52842624e-02 7.00410237e-01 9.07649546e-01\n",
      "  5.39498061e-01 7.47886706e-01 6.45071428e-01 5.48303959e-01\n",
      "  5.27190421e-01 9.01507860e-01 4.17316429e-01 3.65594569e-02\n",
      "  3.38718719e-01 9.08000411e-01 2.73179904e-01 9.48323804e-01\n",
      "  8.56577925e-01 8.25043015e-01 2.38468484e-01 1.90616935e-01\n",
      "  7.62002364e-01 5.53160941e-01 4.15802257e-01 7.38380662e-01]\n",
      " [2.42574755e-02 4.93257963e-02 1.88844741e-01 2.34843490e-01\n",
      "  2.79067459e-01 9.43351794e-01 9.46287526e-01 5.23754574e-01\n",
      "  8.94107386e-01 8.95538491e-01 7.78235382e-01 4.77394593e-01\n",
      "  8.15217280e-01 5.97330710e-01 1.07791290e-01 8.99742740e-01\n",
      "  2.15146559e-01 1.35760876e-02 5.11777784e-01 2.81327714e-01\n",
      "  2.35637669e-01 6.58117795e-02 7.90221247e-01 4.50561728e-01\n",
      "  7.43020971e-01 1.29556564e-01 5.26027091e-01 7.40030707e-01]\n",
      " [4.31203717e-01 1.74816229e-01 5.62884176e-01 7.55270330e-01\n",
      "  8.42184476e-02 9.90540297e-01 2.51338482e-01 5.94559706e-01\n",
      "  7.09054196e-02 9.02938966e-01 6.39302483e-02 5.12657430e-01\n",
      "  8.46525953e-01 4.26616032e-02 1.69310395e-01 9.00205811e-01\n",
      "  4.96576412e-01 1.72293977e-01 5.01224013e-01 5.04569409e-01\n",
      "  4.67747199e-01 5.69092950e-01 4.26879097e-02 5.37925691e-02\n",
      "  8.66617554e-01 8.28291368e-01 4.08303434e-01 1.79362175e-02]\n",
      " [4.75374801e-01 9.40801402e-01 7.04774609e-01 6.42418113e-02\n",
      "  5.47416905e-02 7.90329885e-01 1.07595361e-02 8.49574843e-01\n",
      "  2.22042359e-01 5.75505914e-02 4.05182476e-01 9.58830432e-01\n",
      "  1.54891942e-01 2.79378679e-01 8.46610746e-01 3.91630335e-01\n",
      "  2.68156327e-01 5.79916415e-02 1.18470278e-02 6.18006024e-01\n",
      "  8.02052609e-01 1.55164646e-01 1.05972710e-01 4.65433999e-01\n",
      "  1.74403894e-01 5.27949227e-01 2.36284229e-01 4.06998065e-01]\n",
      " [1.74997625e-01 7.24056973e-02 8.30043228e-01 9.24905891e-01\n",
      "  2.14663127e-01 5.40722676e-01 8.49642769e-01 3.95921750e-01\n",
      "  8.17966545e-01 6.05569717e-01 9.84285054e-01 6.28095058e-02\n",
      "  9.01448440e-01 2.28142650e-01 8.69639871e-02 1.57160151e-01\n",
      "  1.42002699e-01 6.30033175e-01 7.88763953e-01 8.78562986e-01\n",
      "  8.62436116e-01 9.28329108e-01 2.64641357e-01 6.72461142e-01\n",
      "  8.97257280e-01 4.44862319e-01 1.46962738e-01 2.92066246e-01]\n",
      " [6.06437039e-01 4.72896974e-01 5.93515381e-01 4.46769995e-01\n",
      "  2.08188523e-01 3.56552745e-01 5.38301091e-01 8.18863561e-01\n",
      "  1.74855375e-01 5.79875929e-01 8.38260952e-01 6.59717754e-01\n",
      "  7.52979650e-01 7.31226701e-01 2.23858147e-01 1.22862415e-01\n",
      "  6.11996128e-01 2.52509961e-01 8.04004269e-01 8.42202246e-01\n",
      "  7.93079610e-01 8.59829082e-02 5.66929246e-01 2.62008282e-01\n",
      "  6.28878973e-01 5.61418504e-01 4.81819033e-01 9.82782724e-01]\n",
      " [3.77422367e-01 3.54656356e-01 9.96807100e-01 4.84392505e-01\n",
      "  7.09875796e-01 6.32712030e-01 2.77871639e-01 1.14531728e-01\n",
      "  3.70465515e-01 7.61972151e-01 2.64953601e-01 6.84216740e-01\n",
      "  8.77948034e-01 1.87903619e-01 3.36896305e-01 8.04404712e-02\n",
      "  5.42148005e-01 4.81046943e-01 5.91299602e-01 1.30183392e-01\n",
      "  8.21280802e-01 4.18317357e-01 7.87186700e-01 1.12273131e-01\n",
      "  5.58620632e-01 3.60876762e-01 6.82034413e-01 5.95543055e-01]\n",
      " [9.55582911e-01 4.46571164e-01 6.00542048e-01 2.68042703e-01\n",
      "  1.97517576e-01 8.08676826e-01 6.30463841e-01 7.46699383e-01\n",
      "  6.70498910e-01 1.75550268e-01 7.12461118e-01 5.14729936e-01\n",
      "  9.23236969e-01 4.62313199e-01 2.30961305e-01 8.46487407e-01\n",
      "  8.52071336e-01 1.34507883e-01 8.24288961e-01 1.11533323e-01\n",
      "  4.62981094e-01 5.62700924e-01 1.25419273e-01 9.62424897e-01\n",
      "  6.62495302e-01 7.96117161e-01 7.30691978e-01 1.58689279e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[0, 0]) # 访问第1个数据的第1个通道的空间数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.4.2  基于 im2col 的展开\n",
    "\n",
    "NumPy中存在使用 for语句后处理变慢的缺点（NumPy中，访问元素时最好不要用 for语句）\n",
    "\n",
    "im2col是一个函数，将输入数据展开以适合滤波器（权重）。如图7-17所示，对3维的输入数据应用 im2col后，数据转换为2维矩阵（正确地讲，是把包含批数量的4维数据转换成了2维数据）。\n",
    "\n",
    "![jupyter](./images/im2col的示意图.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。 im2col会在所有应用滤波器的地方进行这个展开处理。\n",
    "\n",
    "![jupyter](./images/将滤波器的应用区域从头开始依次横向展开为1列.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在滤波器的应用区域重叠的情况下，使用 im2col展开后，展开后的元素个数会多于原方块的元素个数。因此，使用 im2col的实现存在比普通的实现消耗更多内存的缺点。\n",
    "\n",
    "但是，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处。比如，在矩阵计算的库（线性代数库）等中，矩阵计算的实现已被高度最优化，可以高速地进行大矩阵的乘法运算。因此，通过归结到矩阵计算上，可以有效地利用线性代数库。\n",
    "\n",
    "im2col 是“image to column”的缩写，翻译过来就是“从图像到矩阵”的意思。\n",
    "\n",
    "使用 im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可（参照图7-19）。这和全连接层的Affine层进行的处理基本相同。\n",
    "\n",
    "![jupyter](./images/卷积运算的滤波器处理的细节.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.4.3  卷积层的实现\n",
    "\n",
    "im2col这一便捷函数具有以下接口。im2col (input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "\n",
    "    • input_data―由（数据量，通道，高，长）的4维数组构成的输入数据\n",
    "    • filter_h―滤波器的高\n",
    "    • filter_w―滤波器的长\n",
    "    • stride―步幅\n",
    "    • pad―填充\n",
    "\n",
    "im2col会考虑滤波器大小、 步幅、填充，将输入数据展开为2维数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im2col 函数的实现\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n",
    "    filter_h : 滤波器的高\n",
    "    filter_w : 滤波器的长\n",
    "    stride : 步幅\n",
    "    pad : 填充\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2维数组\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = int((H + 2 * pad - filter_h) / stride + 1)\n",
    "    out_w = int((W + 2 * pad - filter_w) / stride + 1)\n",
    "    \n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :,] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "            \n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col2im 函数的实现\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# 使用 im2col 函数\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7) # 批大小为1、通道为3的7 × 7的数据\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # (9,75)\n",
    "# OH = (7+2*0-5)/1+1=3\n",
    "# OW = (7+2*0-5)/1+1=3\n",
    "# 1*3*3 = 9\n",
    "# 第2维的元素个数为75。这是滤波器（通道为3、大小为(5 × 5)的元素个数的总和\n",
    "\n",
    "x2 = np.random.rand(10,3,7,7) # 批大小为10、通道为3的7 × 7的数据\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) # (10,75)\n",
    "# OH = (7+2*0-5)/1+1=3\n",
    "# OW = (7+2*0-5)/1+1=3\n",
    "# 10*3*3 = 90\n",
    "# 第2维的元素个数为75。这是滤波器（通道为3、大小为(5 × 5)的元素个数的总和\n",
    "# 批大小为1时， im2col的结果是 (9, 75)。而第2个例子中批大小为10，所以保存了10倍的数据，即 (90, 75)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用im2col来实现卷积层 Convolution 类\n",
    "\n",
    "class Convolution:\n",
    "    \n",
    "    # 初始化\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        # 初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中间数据(backward 时常用)\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 权重和偏置参数的梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    # 正向转播\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape # 滤波器是 (FN, C, FH, FW)的 4 维形状\n",
    "        N, C, H, W = x.shape # 输入 4维数组\n",
    "        \n",
    "        # 输出大小\n",
    "        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # 通过im2col把输入数据展开以适应滤波器（权重）\n",
    "        \n",
    "        # 通过在 reshape时指定为 -1， reshape函数会自动计算 -1维度上的元素个数，以使多维数组的元素个数前后一致。\n",
    "        # 比如，(10, 3, 5, 5)形状的数组的元素个数共有750个，指定 reshape(10,-1)后，就会转换成(10, 75)形状的数组。\n",
    "        col_W = self.W.reshape(FN, -1).T # 滤波器的展开\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b # 数据和权重进行点积，后加上偏置\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2) # transpose会更改多维数组的轴的顺序\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # 反向传播\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1,0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, slef.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, slef.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.4.4  池化层的实现\n",
    "\n",
    "池化层的实现和卷积层相同，都使用 im2col展开输入数据。不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。具体地讲，如图7-21所示，池化的应用区域按通道单独展开。\n",
    "\n",
    "![jupyter](./images/对输入数据展开池化的应用区域.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像这样展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可（图7-22）。\n",
    "\n",
    "![jupyter](./images/池化层的实现流程.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化层 Pooling 的实现\n",
    "\n",
    "class Pooling:\n",
    "    \n",
    "    # 初始化\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    # 正向传播\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展开\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1) # np.argmax()返回某维度中最大值的下标\n",
    "        \n",
    "        # 最大值\n",
    "        # np.max 可以指定axis参数，并在这个参数指定的各个轴方向上求最大值。\n",
    "        # 比如，如果写成 np.max(x, axis=1)，就可以在输入 x的第 1 维的各个轴方向上求最大值。\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        # reshape转换\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # 反向传播\n",
    "    def backwward(self, dout):\n",
    "        dout = dout.transpose(0,2,3,1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5  CNN的实现\n",
    "\n",
    "将已实现的卷积层和池化层进行组合，搭建进行手写数字识别的CNN。\n",
    "\n",
    "![jupyter](./images/简单CNN的网络构成.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图7-23所示，网络的构成是“Convolution - ReLU - Pooling -Afne -ReLU - Afne - Softmax”，我们将它实现为名为 SimpleConvNet的类。\n",
    "\n",
    "首先来看一下 SimpleConvNet的初始化（__init__），取下面这些参数。\n",
    "\n",
    "    • input_dim―输入数据的维度：（通道，高，长）\n",
    "    • conv_param―卷积层的超参数（字典）。字典的关键字如下：\n",
    "        filter_num―滤波器的数量\n",
    "        filter_size―滤波器的大小\n",
    "        stride―步幅\n",
    "        pad―填充\n",
    "    • hidden_size―隐藏层（全连接）的神经元数量\n",
    "    • output_size―输出层（全连接）的神经元数量\n",
    "    • weitght_int_std―初始化时权重的标准差\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleConvNet 的实现\n",
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"\n",
    "    简单的ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 输入大小（MNIST的情况下为784）\n",
    "    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
    "    output_size : 输出大小（MNIST的情况下为10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
    "        指定'relu'或'he'的情况下设定“He的初始值”\n",
    "        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(1,28,28),\n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stried':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        # 生成必要的层\n",
    "        self.layers = OrderedDict() # 有序字典\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 用于推理的 predict方法从头开始依次调用已添加的层，并将结果传递给下一层。\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"求损失函数\n",
    "        参数x是输入数据、t是教师标签\n",
    "        在求损失函数的 loss方法中，除了使用 predict方法进行的 forward处理之外，\n",
    "        还会继续进行forward处理，直到到达最后的 SoftmaxWithLoss层。\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size = 100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def numerical_gradient(self, x,t):\n",
    "        \"\"\"求梯度（数值微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"求梯度（误差反向传播法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "    \n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        \n",
    "        for i, key in enumerate(['Cov1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key],b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这个 SimpleConvNet学习MNIST数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2997096902801686\n",
      "=== epoch:1, train acc:0.25, test acc:0.235 ===\n",
      "train loss:2.2970342314957666\n",
      "train loss:2.292521428400478\n",
      "train loss:2.2857375061127816\n",
      "train loss:2.27166090849401\n",
      "train loss:2.2584274562890765\n",
      "train loss:2.2390580527365906\n",
      "train loss:2.226778851620576\n",
      "train loss:2.1917274869142283\n",
      "train loss:2.1699402651164754\n",
      "train loss:2.152530620887449\n",
      "train loss:2.1349518254613464\n",
      "train loss:2.0721705994793695\n",
      "train loss:2.045369149999405\n",
      "train loss:1.978388983726247\n",
      "train loss:1.89550644352153\n",
      "train loss:1.8321255018818372\n",
      "train loss:1.7380714702449815\n",
      "train loss:1.7322837670709894\n",
      "train loss:1.5916774525338138\n",
      "train loss:1.5562112694505603\n",
      "train loss:1.5535942622962498\n",
      "train loss:1.4039624731526295\n",
      "train loss:1.2198569338986374\n",
      "train loss:1.2026896613626585\n",
      "train loss:1.1030143325432549\n",
      "train loss:1.139401255633115\n",
      "train loss:1.1016680684823503\n",
      "train loss:1.0851370597032461\n",
      "train loss:1.0077290549581273\n",
      "train loss:0.998289927520554\n",
      "train loss:0.7694507487912375\n",
      "train loss:0.8767642187483685\n",
      "train loss:0.7045284587057355\n",
      "train loss:0.7292003820728962\n",
      "train loss:0.7135174440547811\n",
      "train loss:0.6492067786332963\n",
      "train loss:0.8148985340843538\n",
      "train loss:0.7225411634207275\n",
      "train loss:0.7577061504579086\n",
      "train loss:0.5447897401279281\n",
      "train loss:0.623331783891509\n",
      "train loss:0.6739559841099624\n",
      "train loss:0.678208128513674\n",
      "train loss:0.5348911406981138\n",
      "train loss:0.6010137784530264\n",
      "train loss:0.5774566378666821\n",
      "train loss:0.5657043702658882\n",
      "train loss:0.6106022979284308\n",
      "train loss:0.5057482687979806\n",
      "train loss:0.6365403948460495\n",
      "=== epoch:2, train acc:0.806, test acc:0.781 ===\n",
      "train loss:0.47852202490361795\n",
      "train loss:0.47328402235028333\n",
      "train loss:0.5606845746954994\n",
      "train loss:0.48725577945168647\n",
      "train loss:0.4225943558775278\n",
      "train loss:0.5176021879457153\n",
      "train loss:0.6170379244606989\n",
      "train loss:0.4519445275788292\n",
      "train loss:0.5518810739204428\n",
      "train loss:0.34165467893792006\n",
      "train loss:0.35887361545217233\n",
      "train loss:0.47249943168766956\n",
      "train loss:0.5924202088156075\n",
      "train loss:0.5033721005621916\n",
      "train loss:0.5974630032684245\n",
      "train loss:0.6029189603525106\n",
      "train loss:0.459964960661724\n",
      "train loss:0.6009441517227716\n",
      "train loss:0.3536230438846216\n",
      "train loss:0.40759199939093266\n",
      "train loss:0.39124515181754205\n",
      "train loss:0.5601755868845566\n",
      "train loss:0.33674765853615746\n",
      "train loss:0.5030119102814009\n",
      "train loss:0.4363307098950466\n",
      "train loss:0.39901820764047685\n",
      "train loss:0.3576434930610861\n",
      "train loss:0.3451200428792005\n",
      "train loss:0.264175697867826\n",
      "train loss:0.4071135670265204\n",
      "train loss:0.39868349833304206\n",
      "train loss:0.395234829757184\n",
      "train loss:0.36706732424985156\n",
      "train loss:0.35242370046552957\n",
      "train loss:0.27793911268119764\n",
      "train loss:0.41325973636266755\n",
      "train loss:0.46218932817322556\n",
      "train loss:0.29183735765049884\n",
      "train loss:0.48661735080441226\n",
      "train loss:0.3707591469089284\n",
      "train loss:0.3349989651438649\n",
      "train loss:0.38491969446892205\n",
      "train loss:0.4260453364582622\n",
      "train loss:0.3825490613130327\n",
      "train loss:0.32553781137989274\n",
      "train loss:0.23938361621943108\n",
      "train loss:0.505724635258607\n",
      "train loss:0.34841272980210325\n",
      "train loss:0.5036373895923082\n",
      "train loss:0.4071160830461352\n",
      "=== epoch:3, train acc:0.884, test acc:0.872 ===\n",
      "train loss:0.4451316499510515\n",
      "train loss:0.24850916478551835\n",
      "train loss:0.39008216474884494\n",
      "train loss:0.23918545571552816\n",
      "train loss:0.3946797940611995\n",
      "train loss:0.4208120969603574\n",
      "train loss:0.3334060134270627\n",
      "train loss:0.24699585212107922\n",
      "train loss:0.5554252468282672\n",
      "train loss:0.2907131190480658\n",
      "train loss:0.47656019950148737\n",
      "train loss:0.4326876506581802\n",
      "train loss:0.6173081584524608\n",
      "train loss:0.3968084694632356\n",
      "train loss:0.1898089722240378\n",
      "train loss:0.3048840909004628\n",
      "train loss:0.30716233604305465\n",
      "train loss:0.47332064020547954\n",
      "train loss:0.2584255770204593\n",
      "train loss:0.2418014183362235\n",
      "train loss:0.28996750471139254\n",
      "train loss:0.25760269953272535\n",
      "train loss:0.2742425968737899\n",
      "train loss:0.23648077328496328\n",
      "train loss:0.35589784981371686\n",
      "train loss:0.20942733545145406\n",
      "train loss:0.3244846005091221\n",
      "train loss:0.43597684633288203\n",
      "train loss:0.2575395063666021\n",
      "train loss:0.364031477001572\n",
      "train loss:0.2461005955029384\n",
      "train loss:0.2925451763310459\n",
      "train loss:0.30853136431492995\n",
      "train loss:0.2969873220539007\n",
      "train loss:0.34992626945864513\n",
      "train loss:0.31378506958717434\n",
      "train loss:0.27162526006171456\n",
      "train loss:0.4666120696309596\n",
      "train loss:0.3173924469124538\n",
      "train loss:0.23818069393241814\n",
      "train loss:0.2465548688419294\n",
      "train loss:0.30237584692922814\n",
      "train loss:0.19990535225068495\n",
      "train loss:0.18964072601999185\n",
      "train loss:0.600719548397841\n",
      "train loss:0.25318119107624804\n",
      "train loss:0.20055399522236278\n",
      "train loss:0.2930884253731288\n",
      "train loss:0.26652417395060757\n",
      "train loss:0.1856578432801252\n",
      "=== epoch:4, train acc:0.895, test acc:0.877 ===\n",
      "train loss:0.25900598398176555\n",
      "train loss:0.3145815536025975\n",
      "train loss:0.3219780192258772\n",
      "train loss:0.21726082188282997\n",
      "train loss:0.18170893981160027\n",
      "train loss:0.28769577387559503\n",
      "train loss:0.34667321028625997\n",
      "train loss:0.21656207562147464\n",
      "train loss:0.1601310663147171\n",
      "train loss:0.20314034273789272\n",
      "train loss:0.21375161795527958\n",
      "train loss:0.22368006402892873\n",
      "train loss:0.31617906434637333\n",
      "train loss:0.17640410784102525\n",
      "train loss:0.36762270485290605\n",
      "train loss:0.3500858837177327\n",
      "train loss:0.31870570913330765\n",
      "train loss:0.09174306642891344\n",
      "train loss:0.3392608132519723\n",
      "train loss:0.2877695175881196\n",
      "train loss:0.33196977116826754\n",
      "train loss:0.3998938524454271\n",
      "train loss:0.1509256442553467\n",
      "train loss:0.2736046968210155\n",
      "train loss:0.25740773021829466\n",
      "train loss:0.2108849655488378\n",
      "train loss:0.26731188437366543\n",
      "train loss:0.2803927179065331\n",
      "train loss:0.3022605215662596\n",
      "train loss:0.19356318731157474\n",
      "train loss:0.21204435020738213\n",
      "train loss:0.3108405874987939\n",
      "train loss:0.18418307540701495\n",
      "train loss:0.3129535712742679\n",
      "train loss:0.1464672557059927\n",
      "train loss:0.2528685915960638\n",
      "train loss:0.3309968419883262\n",
      "train loss:0.200540516883049\n",
      "train loss:0.35916946145178735\n",
      "train loss:0.33003091703576115\n",
      "train loss:0.21755765892257423\n",
      "train loss:0.1824942315824682\n",
      "train loss:0.277050079862015\n",
      "train loss:0.1598219988863102\n",
      "train loss:0.24943827421288756\n",
      "train loss:0.10965283896866101\n",
      "train loss:0.13971896949700843\n",
      "train loss:0.2625011540002258\n",
      "train loss:0.25234989296899696\n",
      "train loss:0.19048797685237123\n",
      "=== epoch:5, train acc:0.901, test acc:0.901 ===\n",
      "train loss:0.29244858678619673\n",
      "train loss:0.3103093893249865\n",
      "train loss:0.28698721604136\n",
      "train loss:0.28883108679542635\n",
      "train loss:0.295835064770184\n",
      "train loss:0.1942501301478125\n",
      "train loss:0.2672307363146478\n",
      "train loss:0.3049294794907237\n",
      "train loss:0.2777573501316635\n",
      "train loss:0.15900920089858275\n",
      "train loss:0.24167552334508027\n",
      "train loss:0.13385033013438494\n",
      "train loss:0.28924606840425926\n",
      "train loss:0.3342999645830328\n",
      "train loss:0.1726397235953667\n",
      "train loss:0.30889301997783514\n",
      "train loss:0.1858286795444394\n",
      "train loss:0.22930871243053233\n",
      "train loss:0.12949584075552217\n",
      "train loss:0.18723456084885007\n",
      "train loss:0.20234686080606695\n",
      "train loss:0.18230237508870573\n",
      "train loss:0.273873771265607\n",
      "train loss:0.2508064131559415\n",
      "train loss:0.3979125509706956\n",
      "train loss:0.13639901318652292\n",
      "train loss:0.16682233659951223\n",
      "train loss:0.2436801669767028\n",
      "train loss:0.2956191940857041\n",
      "train loss:0.2659678243491412\n",
      "train loss:0.12363074814771374\n",
      "train loss:0.17506244623849784\n",
      "train loss:0.21663506510931177\n",
      "train loss:0.38101698001916645\n",
      "train loss:0.3561693615581442\n",
      "train loss:0.26151215218560947\n",
      "train loss:0.20268790905869022\n",
      "train loss:0.2089994437127958\n",
      "train loss:0.28819522817076915\n",
      "train loss:0.10678702924147146\n",
      "train loss:0.2615416557775511\n",
      "train loss:0.2531171092623746\n",
      "train loss:0.14995913872515312\n",
      "train loss:0.2499310023366904\n",
      "train loss:0.21937293834636193\n",
      "train loss:0.17027715477932237\n",
      "train loss:0.3077009692567066\n",
      "train loss:0.2965831974865308\n",
      "train loss:0.2478172979963645\n",
      "train loss:0.18706379855871463\n",
      "=== epoch:6, train acc:0.905, test acc:0.898 ===\n",
      "train loss:0.23804923220218477\n",
      "train loss:0.2464963206413498\n",
      "train loss:0.2912909480299692\n",
      "train loss:0.2164481758265637\n",
      "train loss:0.1644984615103444\n",
      "train loss:0.14446608040591294\n",
      "train loss:0.2626953947814821\n",
      "train loss:0.2847905710990689\n",
      "train loss:0.23871428170763706\n",
      "train loss:0.20929850802775374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.28664139479154327\n",
      "train loss:0.3065239004677734\n",
      "train loss:0.20776739159242627\n",
      "train loss:0.15664877154234483\n",
      "train loss:0.20667299771104108\n",
      "train loss:0.20489169762731105\n",
      "train loss:0.23665287026485127\n",
      "train loss:0.27609952737284266\n",
      "train loss:0.19654124719247945\n",
      "train loss:0.36529446637115853\n",
      "train loss:0.13496907606536163\n",
      "train loss:0.14439578546066764\n",
      "train loss:0.13503612019477904\n",
      "train loss:0.2288999908312811\n",
      "train loss:0.1859808044222978\n",
      "train loss:0.15649463804880562\n",
      "train loss:0.12626290923912828\n",
      "train loss:0.19072623290068882\n",
      "train loss:0.2730568359546124\n",
      "train loss:0.12187648865846254\n",
      "train loss:0.14883854927668216\n",
      "train loss:0.27024215763923515\n",
      "train loss:0.16773696376943628\n",
      "train loss:0.1984669962268649\n",
      "train loss:0.22962998096493284\n",
      "train loss:0.15563328635576543\n",
      "train loss:0.14548780780147796\n",
      "train loss:0.19275603707190336\n",
      "train loss:0.07489029919816398\n",
      "train loss:0.23907971675095344\n",
      "train loss:0.20887355301786684\n",
      "train loss:0.11619034027874277\n",
      "train loss:0.13716751729845153\n",
      "train loss:0.21211372475556747\n",
      "train loss:0.26293305080620666\n",
      "train loss:0.1661039023701881\n",
      "train loss:0.15843157775780287\n",
      "train loss:0.1534644431025907\n",
      "train loss:0.16371544484294226\n",
      "train loss:0.10288862690793923\n",
      "=== epoch:7, train acc:0.935, test acc:0.91 ===\n",
      "train loss:0.1328329676696782\n",
      "train loss:0.1499586866098276\n",
      "train loss:0.30928066793741665\n",
      "train loss:0.1610289941704116\n",
      "train loss:0.21228150617532446\n",
      "train loss:0.08507107170901737\n",
      "train loss:0.1379341047408259\n",
      "train loss:0.21786729036490912\n",
      "train loss:0.08701758179620063\n",
      "train loss:0.22703710687108242\n",
      "train loss:0.12637833576488547\n",
      "train loss:0.13921331208205642\n",
      "train loss:0.11875083644118353\n",
      "train loss:0.1988191978845178\n",
      "train loss:0.12428344797864325\n",
      "train loss:0.12458184750533369\n",
      "train loss:0.12928021855913177\n",
      "train loss:0.1373880366314997\n",
      "train loss:0.20789079942277167\n",
      "train loss:0.1368513057861044\n",
      "train loss:0.2386934798741662\n",
      "train loss:0.1285770317634958\n",
      "train loss:0.1560394080351947\n",
      "train loss:0.2011204422736948\n",
      "train loss:0.1363555701520351\n",
      "train loss:0.22099826897966732\n",
      "train loss:0.12458093625988058\n",
      "train loss:0.22933002130795063\n",
      "train loss:0.14407399907405913\n",
      "train loss:0.10014313540576104\n",
      "train loss:0.16952329946025496\n",
      "train loss:0.13797481055586985\n",
      "train loss:0.1785513798803794\n",
      "train loss:0.11798685641664491\n",
      "train loss:0.0912018166551833\n",
      "train loss:0.16517947378488768\n",
      "train loss:0.06531388478602397\n",
      "train loss:0.12055144430365099\n",
      "train loss:0.09305162757220055\n",
      "train loss:0.08710888268406683\n",
      "train loss:0.11703890057635186\n",
      "train loss:0.15054095767526263\n",
      "train loss:0.08500886204373384\n",
      "train loss:0.06499996735163538\n",
      "train loss:0.09839736426421522\n",
      "train loss:0.11286152131611661\n",
      "train loss:0.12316349995950202\n",
      "train loss:0.4445159743623105\n",
      "train loss:0.14574012833082242\n",
      "train loss:0.20878656668998286\n",
      "=== epoch:8, train acc:0.947, test acc:0.919 ===\n",
      "train loss:0.1785089231175569\n",
      "train loss:0.1339874497525613\n",
      "train loss:0.1707563516312298\n",
      "train loss:0.1262902265563244\n",
      "train loss:0.14526822774077364\n",
      "train loss:0.214934142160825\n",
      "train loss:0.20823518666370067\n",
      "train loss:0.14358188094469984\n",
      "train loss:0.21913730184855848\n",
      "train loss:0.23241044911996872\n",
      "train loss:0.11744314219162705\n",
      "train loss:0.31199122723724915\n",
      "train loss:0.15901768108390985\n",
      "train loss:0.2044345273346457\n",
      "train loss:0.16770645511033766\n",
      "train loss:0.16072789312759583\n",
      "train loss:0.0797950730132083\n",
      "train loss:0.2077135941385456\n",
      "train loss:0.21752803776038068\n",
      "train loss:0.16060833425912896\n",
      "train loss:0.11564581186258921\n",
      "train loss:0.16307474012582054\n",
      "train loss:0.1665190300725765\n",
      "train loss:0.10408510422837541\n",
      "train loss:0.08476707559601629\n",
      "train loss:0.21213110324745668\n",
      "train loss:0.07382875404902384\n",
      "train loss:0.1338188174621745\n",
      "train loss:0.11712846989993318\n",
      "train loss:0.11774068624859446\n",
      "train loss:0.20349616880192367\n",
      "train loss:0.11995071742515659\n",
      "train loss:0.30758942192815253\n",
      "train loss:0.15268054912396514\n",
      "train loss:0.1071901344600623\n",
      "train loss:0.13348072688192203\n",
      "train loss:0.1278928320818795\n",
      "train loss:0.08864011893275084\n",
      "train loss:0.10771498653014923\n",
      "train loss:0.08831219628535555\n",
      "train loss:0.13998162769609038\n",
      "train loss:0.07863273491072843\n",
      "train loss:0.21778090403715367\n",
      "train loss:0.1698285803439028\n",
      "train loss:0.09341063484492583\n",
      "train loss:0.23593499526161058\n",
      "train loss:0.24555733562821966\n",
      "train loss:0.05025543524298218\n",
      "train loss:0.1561836553211559\n",
      "train loss:0.1571487828371674\n",
      "=== epoch:9, train acc:0.941, test acc:0.929 ===\n",
      "train loss:0.16411134702320102\n",
      "train loss:0.209422406850393\n",
      "train loss:0.06591313842982797\n",
      "train loss:0.22950243377175158\n",
      "train loss:0.08110343247258736\n",
      "train loss:0.09826955145854971\n",
      "train loss:0.13774155966330234\n",
      "train loss:0.08370068702874826\n",
      "train loss:0.1358523590877466\n",
      "train loss:0.16496937469088066\n",
      "train loss:0.06836866587598389\n",
      "train loss:0.14516275031792616\n",
      "train loss:0.1271611240339353\n",
      "train loss:0.06365921412013588\n",
      "train loss:0.19026415564869603\n",
      "train loss:0.10858518741117626\n",
      "train loss:0.09329000103516857\n",
      "train loss:0.1570193245554432\n",
      "train loss:0.19549614073013288\n",
      "train loss:0.1699059175584878\n",
      "train loss:0.11842897745047078\n",
      "train loss:0.10853448704651555\n",
      "train loss:0.11142514240171694\n",
      "train loss:0.10645353825278006\n",
      "train loss:0.21603897310404385\n",
      "train loss:0.21571688292363528\n",
      "train loss:0.17436176576852314\n",
      "train loss:0.13679222578704014\n",
      "train loss:0.35370942429873464\n",
      "train loss:0.1294181883933469\n",
      "train loss:0.16251111864036719\n",
      "train loss:0.1666061629602938\n",
      "train loss:0.06180573457614641\n",
      "train loss:0.07439869125023503\n",
      "train loss:0.06418598746972996\n",
      "train loss:0.057030829472955485\n",
      "train loss:0.24379922736752982\n",
      "train loss:0.09668208788575074\n",
      "train loss:0.10451210484614729\n",
      "train loss:0.07754785327436303\n",
      "train loss:0.0708347231345216\n",
      "train loss:0.11848070868758563\n",
      "train loss:0.05890420433459638\n",
      "train loss:0.10248006245654873\n",
      "train loss:0.10300661925742537\n",
      "train loss:0.05846122920305669\n",
      "train loss:0.14144556571439065\n",
      "train loss:0.14820833634656422\n",
      "train loss:0.10673110298978099\n",
      "train loss:0.1181550394664323\n",
      "=== epoch:10, train acc:0.949, test acc:0.923 ===\n",
      "train loss:0.06465272565489844\n",
      "train loss:0.14728918792808915\n",
      "train loss:0.029354735518515883\n",
      "train loss:0.09514559051631387\n",
      "train loss:0.13854005816368523\n",
      "train loss:0.18781531505453747\n",
      "train loss:0.16693734593621426\n",
      "train loss:0.07546305730858228\n",
      "train loss:0.11026842986117268\n",
      "train loss:0.13581351533628075\n",
      "train loss:0.09955250521539012\n",
      "train loss:0.0525141520163921\n",
      "train loss:0.09423975825526933\n",
      "train loss:0.163630582627976\n",
      "train loss:0.11597657933018432\n",
      "train loss:0.09632682995803708\n",
      "train loss:0.11233442231935575\n",
      "train loss:0.12726188418240228\n",
      "train loss:0.054460913742732285\n",
      "train loss:0.12540717674594024\n",
      "train loss:0.10795866014526853\n",
      "train loss:0.11524641656248781\n",
      "train loss:0.11067339793477997\n",
      "train loss:0.12667361309338607\n",
      "train loss:0.18084374970071523\n",
      "train loss:0.14876537835043205\n",
      "train loss:0.08356705305265655\n",
      "train loss:0.13466262901746037\n",
      "train loss:0.08144420614164094\n",
      "train loss:0.08520771738542195\n",
      "train loss:0.05817206939781763\n",
      "train loss:0.11841957886560123\n",
      "train loss:0.0731167379892479\n",
      "train loss:0.10895043671108733\n",
      "train loss:0.12116192107595754\n",
      "train loss:0.053237448926279594\n",
      "train loss:0.1617621927781856\n",
      "train loss:0.07731024167864095\n",
      "train loss:0.07105168409712424\n",
      "train loss:0.08776300571284495\n",
      "train loss:0.10326015735443014\n",
      "train loss:0.06021539834337603\n",
      "train loss:0.10019596267413089\n",
      "train loss:0.08224640519976452\n",
      "train loss:0.16642127320021935\n",
      "train loss:0.09818585759903935\n",
      "train loss:0.12245940958557038\n",
      "train loss:0.05662549564812216\n",
      "train loss:0.0937993914543429\n",
      "train loss:0.08351246599128158\n",
      "=== epoch:11, train acc:0.964, test acc:0.942 ===\n",
      "train loss:0.10591945964923415\n",
      "train loss:0.14606194074895104\n",
      "train loss:0.08408888021065852\n",
      "train loss:0.12081160238526469\n",
      "train loss:0.13341445316725836\n",
      "train loss:0.12291465723799265\n",
      "train loss:0.06228717848196249\n",
      "train loss:0.07755080419382251\n",
      "train loss:0.07104117595976966\n",
      "train loss:0.03640204960722695\n",
      "train loss:0.1175727260863397\n",
      "train loss:0.182247522509109\n",
      "train loss:0.10829407786839836\n",
      "train loss:0.058949440131871214\n",
      "train loss:0.07521118476984295\n",
      "train loss:0.0971047737119965\n",
      "train loss:0.11366925226094006\n",
      "train loss:0.07717850235223146\n",
      "train loss:0.05431207593333383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.103636579822234\n",
      "train loss:0.1341324632954491\n",
      "train loss:0.14737592998285998\n",
      "train loss:0.1020047295825459\n",
      "train loss:0.08588306969505485\n",
      "train loss:0.08352083222735035\n",
      "train loss:0.054597661984834865\n",
      "train loss:0.11105726531568909\n",
      "train loss:0.13591349328503355\n",
      "train loss:0.15717126683242416\n",
      "train loss:0.05474747629587058\n",
      "train loss:0.07047038696047254\n",
      "train loss:0.11151059133210575\n",
      "train loss:0.08685463703304568\n",
      "train loss:0.056775355698421084\n",
      "train loss:0.11206571313462779\n",
      "train loss:0.11663767647955728\n",
      "train loss:0.21477732840931346\n",
      "train loss:0.1154417170922695\n",
      "train loss:0.06473478269123141\n",
      "train loss:0.07518866606742297\n",
      "train loss:0.05326128639911658\n",
      "train loss:0.19631570608637028\n",
      "train loss:0.04464580561870787\n",
      "train loss:0.08306095107792467\n",
      "train loss:0.09717003978371959\n",
      "train loss:0.07268757426191928\n",
      "train loss:0.04396230511916387\n",
      "train loss:0.10659297586530721\n",
      "train loss:0.05297798315407053\n",
      "train loss:0.07221956262409321\n",
      "=== epoch:12, train acc:0.964, test acc:0.944 ===\n",
      "train loss:0.07834121356683438\n",
      "train loss:0.11092778214478013\n",
      "train loss:0.08833387481786607\n",
      "train loss:0.10340938057013278\n",
      "train loss:0.047510294234971814\n",
      "train loss:0.14080740989750243\n",
      "train loss:0.04975742232024916\n",
      "train loss:0.05801139559385926\n",
      "train loss:0.09001803239366427\n",
      "train loss:0.123344642652991\n",
      "train loss:0.06081555634215328\n",
      "train loss:0.0909256526148233\n",
      "train loss:0.15418548305708257\n",
      "train loss:0.047872936980386\n",
      "train loss:0.11503297032203637\n",
      "train loss:0.08842215308943623\n",
      "train loss:0.08396793864617827\n",
      "train loss:0.044454372427863396\n",
      "train loss:0.11730399668831543\n",
      "train loss:0.035697503177764496\n",
      "train loss:0.04276283953837234\n",
      "train loss:0.06272313327362367\n",
      "train loss:0.05934950706138725\n",
      "train loss:0.1143743795151623\n",
      "train loss:0.02620599221781943\n",
      "train loss:0.051490099486322286\n",
      "train loss:0.11265962602513367\n",
      "train loss:0.07907505172709124\n",
      "train loss:0.08544428783083027\n",
      "train loss:0.11858613320493934\n",
      "train loss:0.16282272514911342\n",
      "train loss:0.037419736013595634\n",
      "train loss:0.022658984491862783\n",
      "train loss:0.09608958236018532\n",
      "train loss:0.0876238533679212\n",
      "train loss:0.049634783645522026\n",
      "train loss:0.0727312406112644\n",
      "train loss:0.052704246525939885\n",
      "train loss:0.023360899176629247\n",
      "train loss:0.05616434819491844\n",
      "train loss:0.06750085310375964\n",
      "train loss:0.10066772922816879\n",
      "train loss:0.05117075243400971\n",
      "train loss:0.060477804342770176\n",
      "train loss:0.1508421641842944\n",
      "train loss:0.10839151932379387\n",
      "train loss:0.08164303427274262\n",
      "train loss:0.0677345636707512\n",
      "train loss:0.06027724495362663\n",
      "train loss:0.08861742107159752\n",
      "=== epoch:13, train acc:0.966, test acc:0.943 ===\n",
      "train loss:0.10645150276584646\n",
      "train loss:0.11058841795173015\n",
      "train loss:0.07618193992372645\n",
      "train loss:0.11989493784634432\n",
      "train loss:0.09642364256073434\n",
      "train loss:0.034251856874604865\n",
      "train loss:0.05211178369741039\n",
      "train loss:0.11904887252464985\n",
      "train loss:0.046610698861892716\n",
      "train loss:0.02194770334555862\n",
      "train loss:0.0953743402894096\n",
      "train loss:0.0761703043187417\n",
      "train loss:0.19522492593083263\n",
      "train loss:0.018172382577331374\n",
      "train loss:0.09159850087125589\n",
      "train loss:0.09710762099411385\n",
      "train loss:0.1555982953447907\n",
      "train loss:0.1660157875139256\n",
      "train loss:0.07880612417418516\n",
      "train loss:0.07035293374645342\n",
      "train loss:0.06472664161681285\n",
      "train loss:0.0754091168912116\n",
      "train loss:0.04920945298069308\n",
      "train loss:0.0754417342668276\n",
      "train loss:0.03754637286080025\n",
      "train loss:0.06507537103308479\n",
      "train loss:0.04844745356709196\n",
      "train loss:0.050541103705962126\n",
      "train loss:0.05884649954480331\n",
      "train loss:0.03641053400478375\n",
      "train loss:0.11815625079141114\n",
      "train loss:0.08825092508760862\n",
      "train loss:0.0935255834204779\n",
      "train loss:0.10779104580923222\n",
      "train loss:0.1426258591256446\n",
      "train loss:0.07219457175225488\n",
      "train loss:0.055615126428223347\n",
      "train loss:0.07164070614893546\n",
      "train loss:0.06288054465383942\n",
      "train loss:0.04582990305068152\n",
      "train loss:0.10117500691434829\n",
      "train loss:0.04553400789349251\n",
      "train loss:0.09540920272548746\n",
      "train loss:0.1010188134297857\n",
      "train loss:0.07771571223306077\n",
      "train loss:0.09930246142149755\n",
      "train loss:0.07639793077173833\n",
      "train loss:0.12746383310945622\n",
      "train loss:0.11645890217433646\n",
      "train loss:0.09713802038682719\n",
      "=== epoch:14, train acc:0.97, test acc:0.95 ===\n",
      "train loss:0.052888326898990944\n",
      "train loss:0.03514553806095581\n",
      "train loss:0.08168591938886394\n",
      "train loss:0.0491113535548512\n",
      "train loss:0.1336431205088983\n",
      "train loss:0.08137332529689104\n",
      "train loss:0.05867769109141587\n",
      "train loss:0.05891299506934332\n",
      "train loss:0.04380664617259932\n",
      "train loss:0.03428776173193711\n",
      "train loss:0.04793874334851112\n",
      "train loss:0.10563868769239396\n",
      "train loss:0.07016579069963542\n",
      "train loss:0.08717588872072995\n",
      "train loss:0.1507723376972692\n",
      "train loss:0.03827344701322335\n",
      "train loss:0.08883125466836278\n",
      "train loss:0.05173588043326603\n",
      "train loss:0.054432672798954106\n",
      "train loss:0.03318868407584115\n",
      "train loss:0.06876935911540694\n",
      "train loss:0.07024907598784826\n",
      "train loss:0.11112310060131778\n",
      "train loss:0.02785977133233404\n",
      "train loss:0.10654246213233418\n",
      "train loss:0.13265085510536165\n",
      "train loss:0.06345142515914909\n",
      "train loss:0.02507359520632003\n",
      "train loss:0.08147700613151844\n",
      "train loss:0.060214854771361265\n",
      "train loss:0.051287950341398825\n",
      "train loss:0.06695698977735379\n",
      "train loss:0.07881108095401554\n",
      "train loss:0.04240523323717372\n",
      "train loss:0.029301390939872683\n",
      "train loss:0.036195776397935545\n",
      "train loss:0.03586283723307145\n",
      "train loss:0.02859238516394048\n",
      "train loss:0.13993840008000893\n",
      "train loss:0.033841135053528086\n",
      "train loss:0.060396054664973456\n",
      "train loss:0.0442492394260792\n",
      "train loss:0.049337990802851686\n",
      "train loss:0.09074431498617916\n",
      "train loss:0.02335338197168436\n",
      "train loss:0.03834271894043222\n",
      "train loss:0.11529661264097815\n",
      "train loss:0.04591331763019357\n",
      "train loss:0.08891441711414945\n",
      "train loss:0.053813941196901964\n",
      "=== epoch:15, train acc:0.978, test acc:0.948 ===\n",
      "train loss:0.08772888250676297\n",
      "train loss:0.037013142901571296\n",
      "train loss:0.08381339163933553\n",
      "train loss:0.16501851678178872\n",
      "train loss:0.10051395076732259\n",
      "train loss:0.054565015311324815\n",
      "train loss:0.059795071342526436\n",
      "train loss:0.03392773435366632\n",
      "train loss:0.058891655255915575\n",
      "train loss:0.06734959403362296\n",
      "train loss:0.10182416684954364\n",
      "train loss:0.059617564467726335\n",
      "train loss:0.04798532160610557\n",
      "train loss:0.019167234515169183\n",
      "train loss:0.04236146426753403\n",
      "train loss:0.03254640255300805\n",
      "train loss:0.056954773955171685\n",
      "train loss:0.13840403215362496\n",
      "train loss:0.07225370978430268\n",
      "train loss:0.022149607869807627\n",
      "train loss:0.08804467076886009\n",
      "train loss:0.07503018296386822\n",
      "train loss:0.06459105613800864\n",
      "train loss:0.03987824147578019\n",
      "train loss:0.08488027598668507\n",
      "train loss:0.06753404035526059\n",
      "train loss:0.0487281963163083\n",
      "train loss:0.07431715955810447\n",
      "train loss:0.03029438654777428\n",
      "train loss:0.06961239208561226\n",
      "train loss:0.09237761805692905\n",
      "train loss:0.03453413777455559\n",
      "train loss:0.03104086812701123\n",
      "train loss:0.03349020596284789\n",
      "train loss:0.02419807537641542\n",
      "train loss:0.07750896799955448\n",
      "train loss:0.04286829180348366\n",
      "train loss:0.060285390296897896\n",
      "train loss:0.05225925552858346\n",
      "train loss:0.038798947958353584\n",
      "train loss:0.05082848613126131\n",
      "train loss:0.02356797824818949\n",
      "train loss:0.10980152123396271\n",
      "train loss:0.024408012000058397\n",
      "train loss:0.06293296678898715\n",
      "train loss:0.024366874651774784\n",
      "train loss:0.07994587704992547\n",
      "train loss:0.04955888840066994\n",
      "train loss:0.04985278515214985\n",
      "train loss:0.036040440271905304\n",
      "=== epoch:16, train acc:0.977, test acc:0.955 ===\n",
      "train loss:0.028506234541619584\n",
      "train loss:0.06688518086453653\n",
      "train loss:0.031778930905137844\n",
      "train loss:0.035900739458159914\n",
      "train loss:0.08223742867440657\n",
      "train loss:0.0368028471309733\n",
      "train loss:0.01017178685296102\n",
      "train loss:0.01692441634255876\n",
      "train loss:0.019950489965149077\n",
      "train loss:0.03794451506296694\n",
      "train loss:0.04925131049813745\n",
      "train loss:0.02885069346618699\n",
      "train loss:0.02011097592994962\n",
      "train loss:0.017031151213073234\n",
      "train loss:0.07413627142117953\n",
      "train loss:0.031011296248481555\n",
      "train loss:0.05090206803685726\n",
      "train loss:0.05957222894779629\n",
      "train loss:0.03542200027212012\n",
      "train loss:0.03980698233898885\n",
      "train loss:0.023837516886539276\n",
      "train loss:0.026020719312197956\n",
      "train loss:0.08854259348858987\n",
      "train loss:0.06893863503073597\n",
      "train loss:0.053800260244905126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.038603243168404494\n",
      "train loss:0.07205090016324715\n",
      "train loss:0.05532306209184199\n",
      "train loss:0.04491892867651907\n",
      "train loss:0.019665125390250577\n",
      "train loss:0.06557418947198458\n",
      "train loss:0.030917579644692793\n",
      "train loss:0.016477209691108406\n",
      "train loss:0.024366257353598698\n",
      "train loss:0.1218408584743589\n",
      "train loss:0.07482562248969937\n",
      "train loss:0.06613859043292587\n",
      "train loss:0.06768600708130446\n",
      "train loss:0.026500804825096716\n",
      "train loss:0.03882365185110095\n",
      "train loss:0.041445592109733205\n",
      "train loss:0.019496253647151646\n",
      "train loss:0.03884659353935769\n",
      "train loss:0.036193182934477236\n",
      "train loss:0.04543321410507742\n",
      "train loss:0.018428947311034904\n",
      "train loss:0.05547751745656237\n",
      "train loss:0.05652465647373221\n",
      "train loss:0.056945499778076177\n",
      "train loss:0.05334507764983761\n",
      "=== epoch:17, train acc:0.972, test acc:0.946 ===\n",
      "train loss:0.020810150235378955\n",
      "train loss:0.02085208240657168\n",
      "train loss:0.08584746186673016\n",
      "train loss:0.08919262014430128\n",
      "train loss:0.020782951476856926\n",
      "train loss:0.027587142382429056\n",
      "train loss:0.06371748557866795\n",
      "train loss:0.045321109100110356\n",
      "train loss:0.10174767372026608\n",
      "train loss:0.03555928374680571\n",
      "train loss:0.04287480701675643\n",
      "train loss:0.058059423039011106\n",
      "train loss:0.03510625739433697\n",
      "train loss:0.03203446951382649\n",
      "train loss:0.017645568572102036\n",
      "train loss:0.058059718033703446\n",
      "train loss:0.051382899854917306\n",
      "train loss:0.06756365839002867\n",
      "train loss:0.10345173091387774\n",
      "train loss:0.032180742178171434\n",
      "train loss:0.05535493532452974\n",
      "train loss:0.0695946466728793\n",
      "train loss:0.018858016353100755\n",
      "train loss:0.04345585997398347\n",
      "train loss:0.05239982452799512\n",
      "train loss:0.03808393378541482\n",
      "train loss:0.0327733130417684\n",
      "train loss:0.01564664516327765\n",
      "train loss:0.08653670272801055\n",
      "train loss:0.03811292176677957\n",
      "train loss:0.04561259898316573\n",
      "train loss:0.03275793327329975\n",
      "train loss:0.057620715854979056\n",
      "train loss:0.037681701570901026\n",
      "train loss:0.027627997314206416\n",
      "train loss:0.02880416660954493\n",
      "train loss:0.026741058304301322\n",
      "train loss:0.046168743607991114\n",
      "train loss:0.02886835756647336\n",
      "train loss:0.035640188857561046\n",
      "train loss:0.04848460168238437\n",
      "train loss:0.016150335411447732\n",
      "train loss:0.046140381786926275\n",
      "train loss:0.09233858412862352\n",
      "train loss:0.0637840313414772\n",
      "train loss:0.10150022494744197\n",
      "train loss:0.07789782882659857\n",
      "train loss:0.04853077666057455\n",
      "train loss:0.0733383213438695\n",
      "train loss:0.057045863922855815\n",
      "=== epoch:18, train acc:0.974, test acc:0.947 ===\n",
      "train loss:0.07983552889689696\n",
      "train loss:0.03949541960180407\n",
      "train loss:0.01703011662760258\n",
      "train loss:0.07172951759925077\n",
      "train loss:0.04009516704567226\n",
      "train loss:0.03794503375221294\n",
      "train loss:0.025000979219188153\n",
      "train loss:0.016423757824255075\n",
      "train loss:0.03745600113927652\n",
      "train loss:0.024442259014320095\n",
      "train loss:0.022989133724006846\n",
      "train loss:0.04123657542116707\n",
      "train loss:0.046808096224326995\n",
      "train loss:0.0863807498291571\n",
      "train loss:0.07143927443146132\n",
      "train loss:0.05064634306874794\n",
      "train loss:0.010155349826109172\n",
      "train loss:0.03544448729791207\n",
      "train loss:0.0464812931684921\n",
      "train loss:0.04743293100881625\n",
      "train loss:0.0852822629678411\n",
      "train loss:0.0759099258477897\n",
      "train loss:0.07598630846730603\n",
      "train loss:0.039990702006777165\n",
      "train loss:0.01724770091792512\n",
      "train loss:0.07580438610348571\n",
      "train loss:0.036873504592167206\n",
      "train loss:0.0301980957711294\n",
      "train loss:0.033987521250806375\n",
      "train loss:0.06682860111902475\n",
      "train loss:0.02229443812070567\n",
      "train loss:0.04671067152356124\n",
      "train loss:0.04115581231221724\n",
      "train loss:0.03397694203977917\n",
      "train loss:0.022137378510501085\n",
      "train loss:0.025173745000658294\n",
      "train loss:0.018844070616206774\n",
      "train loss:0.06342048724240547\n",
      "train loss:0.04559416700231903\n",
      "train loss:0.04372027939189694\n",
      "train loss:0.0658508681063793\n",
      "train loss:0.05327250704400366\n",
      "train loss:0.0666614625854862\n",
      "train loss:0.033659947191779116\n",
      "train loss:0.03532166223195837\n",
      "train loss:0.029606814395799933\n",
      "train loss:0.01625086593379005\n",
      "train loss:0.03308257477062145\n",
      "train loss:0.03672196042613336\n",
      "train loss:0.041316308692456574\n",
      "=== epoch:19, train acc:0.984, test acc:0.951 ===\n",
      "train loss:0.028681670025842435\n",
      "train loss:0.02142632485159902\n",
      "train loss:0.02347268001154273\n",
      "train loss:0.0652329798741852\n",
      "train loss:0.034881539018961134\n",
      "train loss:0.048848078550476036\n",
      "train loss:0.02009922706200959\n",
      "train loss:0.014816945680852771\n",
      "train loss:0.04104428421973592\n",
      "train loss:0.0364981323591751\n",
      "train loss:0.06187201643761498\n",
      "train loss:0.018737907227717815\n",
      "train loss:0.028566077206117934\n",
      "train loss:0.07330681252438062\n",
      "train loss:0.03686111847611295\n",
      "train loss:0.0327003483346094\n",
      "train loss:0.025921612612288293\n",
      "train loss:0.04327449974582857\n",
      "train loss:0.036484496599795524\n",
      "train loss:0.0478300590644587\n",
      "train loss:0.033450390610515975\n",
      "train loss:0.023077185339372145\n",
      "train loss:0.053715662584771985\n",
      "train loss:0.03377787034458768\n",
      "train loss:0.08184192308123961\n",
      "train loss:0.021574652062604657\n",
      "train loss:0.026661806737375483\n",
      "train loss:0.03668898462933711\n",
      "train loss:0.06085941763241172\n",
      "train loss:0.014360378874166644\n",
      "train loss:0.08293025018958033\n",
      "train loss:0.039425934355069005\n",
      "train loss:0.03615102182591007\n",
      "train loss:0.03293630180121691\n",
      "train loss:0.05274264953158507\n",
      "train loss:0.018515341888765316\n",
      "train loss:0.03132451911690725\n",
      "train loss:0.0357767121056645\n",
      "train loss:0.03589130181696594\n",
      "train loss:0.028189876223135655\n",
      "train loss:0.021935190294930164\n",
      "train loss:0.0094279095049773\n",
      "train loss:0.05620116648074272\n",
      "train loss:0.03433854417060212\n",
      "train loss:0.04147179985241684\n",
      "train loss:0.0415216058760507\n",
      "train loss:0.03981036747222515\n",
      "train loss:0.05727667264181849\n",
      "train loss:0.01831008996408448\n",
      "train loss:0.06175660168087112\n",
      "=== epoch:20, train acc:0.983, test acc:0.958 ===\n",
      "train loss:0.07608898042033523\n",
      "train loss:0.008482343008731642\n",
      "train loss:0.04314512020611226\n",
      "train loss:0.016309489503059948\n",
      "train loss:0.011163496610988222\n",
      "train loss:0.027289171349419626\n",
      "train loss:0.07291682361963271\n",
      "train loss:0.013121479034772185\n",
      "train loss:0.016453393472704563\n",
      "train loss:0.042422132782804135\n",
      "train loss:0.009220016838382966\n",
      "train loss:0.04669691972845007\n",
      "train loss:0.035193884001613315\n",
      "train loss:0.04280011066367244\n",
      "train loss:0.0329875587834031\n",
      "train loss:0.03662178305373259\n",
      "train loss:0.02190518626395201\n",
      "train loss:0.022259354952443064\n",
      "train loss:0.043294969070234536\n",
      "train loss:0.03076535764478355\n",
      "train loss:0.015702845477570084\n",
      "train loss:0.07177116043393622\n",
      "train loss:0.021470284319201685\n",
      "train loss:0.07052674216647876\n",
      "train loss:0.047820657201612084\n",
      "train loss:0.028341805966578062\n",
      "train loss:0.051793815084064845\n",
      "train loss:0.025485089795543994\n",
      "train loss:0.05614967674701111\n",
      "train loss:0.02181458998654238\n",
      "train loss:0.01350129812536903\n",
      "train loss:0.010470945849988216\n",
      "train loss:0.01566684811372905\n",
      "train loss:0.015197281933043594\n",
      "train loss:0.021895229230831047\n",
      "train loss:0.010989538631172009\n",
      "train loss:0.019471927747342334\n",
      "train loss:0.036768228737470626\n",
      "train loss:0.03141236694380796\n",
      "train loss:0.023529613334535862\n",
      "train loss:0.006776507221471273\n",
      "train loss:0.02176992760756264\n",
      "train loss:0.0964351122372385\n",
      "train loss:0.009363929951577345\n",
      "train loss:0.011613076491559177\n",
      "train loss:0.012389302232291875\n",
      "train loss:0.023272465611331464\n",
      "train loss:0.009120860540375584\n",
      "train loss:0.059849697889582144\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.955\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用 SimpleConvNet 学习MNIST数据集\n",
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "# from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所述，卷积层和池化层是图像识别中必备的模块。 CNN可以有效读取图像中的某种特性，在手写数字识别中，还可以实现高精度的识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6  CNN 的可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.6.1 第1层权重的可视化\n",
    "\n",
    "图7-24中，学习前的滤波器是随机进行初始化的，所以在黑白的浓淡上没有规律可循，但学习后的滤波器变成了有规律的图像。我们发现，通过学习，滤波器被更新成了有规律的滤波器，比如从白到黑渐变的滤波器、含有块状区域（称为blob）的滤波器等。\n",
    "![jupyter](./images/学习前和学习后的第1层的卷积层的权重.png)\n",
    "![jupyter](./images/对水平方向上和垂直方向上的边缘有响应的滤波器.png)\n",
    "\n",
    "图7-25中显示了选择两个学习完的滤波器对输入图像进行卷积处理时的结果。我们发现“滤波器1”对垂直方向上的边缘有响应，“滤波器2”对水平方向上的边缘有响应。\n",
    "\n",
    "由此可知，卷积层的滤波器会提取边缘或斑块等原始信息。而刚才实现的CNN会将这些原始信息传递给后面的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.6.2　基于分层结构的信息提取\n",
    "\n",
    "![jupyter](./images/CNN的卷积层中提取的信息.png)\n",
    "\n",
    "如果堆叠了多层卷积层，则随着层次加深，提取的信息也愈加复杂、抽象，这是深度学习中很有意思的一个地方。最开始的层对简单的边缘有响应，接下来的层对纹理有响应，再后面的层对更加复杂的物体部件有响应。也就是说，随着层次加深，神经元从简单的形状向“高级”信息变化。换句话说，就像我们理解东西的“含义”一样，响应的对象在逐渐变化。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.7 具有代表性的CNN\n",
    "\n",
    "![jupyter](./images/LeNet的网络结构.png)\n",
    "\n",
    "![jupyter](./images/AlexNet.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
